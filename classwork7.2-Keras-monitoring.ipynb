{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply IMDB data for illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "# Number of words to consider as features\n",
    "max_features = 10000\n",
    "# Cut texts after this number of words \n",
    "# (among top max_features most common words)\n",
    "maxlen = 500\n",
    "\n",
    "# Load the data as lists of integers.\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# This turns our lists of integers\n",
    "# into a 2D integer tensor of shape `(samples, maxlen)`\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ./modelTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "Embed_dim=128\n",
    "_input = Input(shape=(None,))\n",
    "\n",
    "# We specify the maximum input length to our Embedding layer\n",
    "# so we can later flatten the embedded inputs\n",
    "x = layers.Embedding(max_features,Embed_dim, input_length=maxlen)(_input)\n",
    "x = Flatten()(x)\n",
    "predictions= Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "model = Model([_input], predictions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "callbacks_list = [\n",
    "    #Interrupts training when improvement stops\n",
    "    #Monitors the model’s validation accuracy\n",
    "    #Interrupts training when accuracy has stopped improving for more than one epoch (that is, two epochs)\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='acc',\n",
    "            patience=1,\n",
    "        ),\n",
    "    #Saves the current weights after every epoch Path to the destination model file\n",
    "    #These two arguments mean you won’t overwrite the model file unless val_loss has improved, which allows you to keep the best model seen during training.\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath='./modelTrain/model7.2.h1',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "        )]\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=40,\n",
    "                    batch_size=32,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add callback of jumping out of Plateau (taking smaller steps)<br>\n",
    "Custom callback to save activation output of each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "class ActivationLogger(keras.callbacks.Callback):\n",
    "    #Called by the parent model before training, to inform the callback of what model will be calling it\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "        layer_outputs = [layer.output for layer in model.layers]\n",
    "        print (np.shape(layer_outputs))\n",
    "        self.activations_model = keras.models.Model(model.input,layer_outputs)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.validation_data is None:\n",
    "            raise RuntimeError('Requires validation_data.')\n",
    "        validation_sample = self.validation_data[0][0:1]\n",
    "        activations = self.activations_model.predict(validation_sample)\n",
    "        #f = open('./modelTrain/activations_at_epoch_' + str(epoch) + '.npz', 'w')\n",
    "        #np.savez(f, activations)\n",
    "        #f.close()\n",
    "        \n",
    "        print(np.shape(activations[0]))\n",
    "        print(np.shape(activations[1]))\n",
    "        print(np.shape(activations[2]))\n",
    "        print(np.shape(activations[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Model([_input], predictions)\n",
    "\n",
    "\n",
    "callbacks_list = [\n",
    "    #Interrupts training when improvement stops\n",
    "    #Monitors the model’s validation accuracy\n",
    "    #Interrupts training when accuracy has stopped improving for more than one epoch (that is, two epochs)\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='acc',\n",
    "            patience=1,\n",
    "        ),\n",
    "    #Saves the current weights after every epoch Path to the destination model file\n",
    "    #These two arguments mean you won’t overwrite the model file unless val_loss has improved, which allows you to keep the best model seen during training.\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath='./modelTrain/model7.2.h2',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "        ),\n",
    "    #Divides the learning rate by 10 when triggered The callback is triggered after the validation\n",
    "    #loss has stopped improving for 10 epochs.\n",
    "        keras.callbacks.ReduceLROnPlateau(#validation loss\n",
    "            monitor='val_loss',\n",
    "            factor=0.1,\n",
    "            patience=10,\n",
    "            \n",
    "        ),\n",
    "        ActivationLogger()\n",
    "\n",
    "\n",
    "]\n",
    "model2.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model2.fit(x_train, y_train,\n",
    "                    epochs=40,\n",
    "                    batch_size=32,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Convnet model <br>\n",
    "illustrate tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "Embed_dim=128\n",
    "_input = Input(shape=(None,))\n",
    "Output_dim=32\n",
    "Conv_windows=7\n",
    "# We specify the maximum input length to our Embedding layer\n",
    "# so we can later flatten the embedded inputs\n",
    "x = layers.Embedding(max_features,Embed_dim, input_length=maxlen,name=\"embed\")(_input)\n",
    "x = layers.Conv1D(Output_dim,Conv_windows,activation=\"relu\")(x)\n",
    "x= layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(Output_dim,Conv_windows,activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "prediction = layers.Dense(1)(x)\n",
    "model3 = Model(inputs=[_input],outputs=prediction)\n",
    "model3.summary()\n",
    "\n",
    "model3.compile(optimizer=RMSprop(lr=1e-4), # default learning rate is 10^-3, which is too fast, accuracy drops when iterating more steps\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ./model_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model3 = load_model( filepath='./modelTrain/model7.2.Conv1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 151s 8ms/step - loss: 0.3148 - acc: 0.8587 - val_loss: 0.4142 - val_acc: 0.8270\n",
      "Epoch 2/20\n",
      " 1280/20000 [>.............................] - ETA: 2:07 - loss: 0.2902 - acc: 0.8789"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir='./model_log_dir', #Log files will be written at this location.\n",
    "        histogram_freq=1, #Records activation histograms every 1 epoch R\n",
    "        embeddings_freq=1, #ecords embedding data every 1 epoch\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "            filepath='./modelTrain/model7.2.Conv1d',\n",
    "            monitor='val_acc',\n",
    "            save_best_only=True,\n",
    "    )\n",
    "]\n",
    "history = model3.fit(x_train, y_train,\n",
    "                    epochs=20,batch_size=128,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
